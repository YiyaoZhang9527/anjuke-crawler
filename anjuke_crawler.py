"""
å®‰å±…å®¢ç§Ÿæˆ¿ä¿¡æ¯çˆ¬è™«ä¸»ç¨‹åº - çœŸæ­£çš„åˆ—è¡¨â†’è¯¦æƒ…æ‰¹é‡çˆ¬è™«
"""

import asyncio
import csv
import os
from playwright.async_api import async_playwright, Browser, Page
from typing import List, Optional
from datetime import datetime

from config import config
from anti_crawler import anti_crawler
from data_extractor import data_extractor
from list_page_crawler import list_page_crawler
from logger import logger
from utils import handle_errors, retry, StatisticsTracker
from duplicate_checker import duplicate_checker


class AnjukeCrawler:
    """å®‰å±…å®¢çˆ¬è™«ä¸»ç±» - åˆ—è¡¨é¡µâ†’è¯¦æƒ…é¡µæ‰¹é‡çˆ¬å–"""

    def __init__(self):
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.stats = StatisticsTracker()

        # åˆå§‹åŒ–æ–°åŠŸèƒ½æ¨¡å—
        if config.enable_duplicate_check:
            csv_file = config.duplicate_csv_file if config.duplicate_csv_file else config.csv_filename
            self.duplicate_checker = duplicate_checker.__class__(csv_file)
            self.duplicate_checker.enable(True)
        else:
            self.duplicate_checker = None

        # éªŒè¯ç æ—¥å¿—åŠŸèƒ½å·²é›†æˆåˆ°loggerä¸­ï¼Œæ— éœ€å•ç‹¬åˆå§‹åŒ–

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self.start()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.close()

    async def start(self):
        """å¯åŠ¨æµè§ˆå™¨"""
        if self.browser:
            return

        logger.crawler_start()

        # å¯åŠ¨Playwright
        playwright = await async_playwright().start()

        # é…ç½®æµè§ˆå™¨å¯åŠ¨é€‰é¡¹
        launch_options = {
            'headless': config.browser_headless,
            'args': [
                '--no-sandbox',
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--disable-gpu'
            ]
        }

        # æ·»åŠ ä»£ç†é…ç½®
        if config.proxy_list:
            proxy = config.proxy_list[0]
            launch_options['proxy'] = {'server': proxy}

        # å¯åŠ¨æµè§ˆå™¨
        self.browser = await playwright.chromium.launch(**launch_options)

        # åˆ›å»ºæ–°é¡µé¢
        self.page = await self.browser.new_page()

        # è®¾ç½®åçˆ¬è™«
        await anti_crawler.setup_browser_stealth(self.page)

        logger.browser_ready()

    async def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.browser:
            await self.browser.close()
            self.browser = None
            self.page = None
            logger.browser_close()

    async def crawl_from_list_pages(self, max_pages: int = None, max_houses_per_page: int = None) -> bool:
        """ä»æˆ¿æºåˆ—è¡¨é¡µå¼€å§‹é€é¡µå¤„ç† - åŠ è½½ä¸€é¡µåˆ—è¡¨â†’çˆ¬å–è¯¥é¡µæ‰€æœ‰è¯¦æƒ…"""
        if not self.page:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨start()æ–¹æ³•æˆ–ä½¿ç”¨async with")

        # ä½¿ç”¨.envé…ç½®çš„é»˜è®¤å€¼
        if max_pages is None:
            max_pages = config.max_pages
        if max_houses_per_page is None:
            max_houses_per_page = config.max_houses_per_page

        # ç”Ÿæˆåˆ—è¡¨é¡µURL
        list_urls = await list_page_crawler.generate_list_urls(max_pages)
        logger.info(f"å‡†å¤‡é€é¡µå¤„ç† {len(list_urls)} ä¸ªåˆ—è¡¨é¡µï¼Œæ¯é¡µæœ€å¤š{max_houses_per_page}å¥—æˆ¿æº")

        # å‡†å¤‡CSVæ–‡ä»¶
        await self._prepare_csv()

        total_processed = 0

        # é€é¡µå¤„ç†ï¼šåŠ è½½åˆ—è¡¨é¡µâ†’ç«‹å³çˆ¬å–è¯¥é¡µçš„è¯¦æƒ…é¡µ
        for page_num, list_url in enumerate(list_urls, 1):
            # æ£€æŸ¥æ€»æˆ¿æºæ•°é™åˆ¶
            if self.stats.success_count >= config.max_total_houses:
                logger.info(f"å·²è¾¾åˆ°æœ€å¤§æˆ¿æºæ•°é™åˆ¶: {config.max_total_houses}")
                break
            try:
                logger.progress(f"å¤„ç†åˆ—è¡¨é¡µ {page_num}/{len(list_urls)}: {list_url}")

                # æå–å½“å‰é¡µçš„æˆ¿æºé“¾æ¥
                house_links = await list_page_crawler.extract_house_links(
                    self.page, list_url, max_houses_per_page
                )

                if not house_links:
                    logger.warning(f"åˆ—è¡¨é¡µ {page_num} æœªæå–åˆ°æˆ¿æºé“¾æ¥")
                    continue

                logger.info(f"åˆ—è¡¨é¡µ {page_num} æå–åˆ° {len(house_links)} ä¸ªæˆ¿æºé“¾æ¥ï¼Œå¼€å§‹çˆ¬å–è¯¦æƒ…")

                # ç«‹å³çˆ¬å–å½“å‰é¡µçš„æ‰€æœ‰æˆ¿æºè¯¦æƒ…
                page_success_count = await self.crawl_house_batch(house_links)

                total_processed += len(house_links)
                logger.info(f"åˆ—è¡¨é¡µ {page_num} å¤„ç†å®Œæˆï¼Œç´¯è®¡å¤„ç† {total_processed} å¥—æˆ¿æº")

                # é¡µé¢é—´å»¶æ—¶
                if page_num < len(list_urls):
                    await anti_crawler.smart_delay(3)

            except Exception:
                continue  # é”™è¯¯å·²è¢«è£…é¥°å™¨è®°å½•

        logger.crawler_stop(self.stats.get_stats())

        return self.stats.success_count > 0

    @handle_errors(default_return=False)
    async def crawl_single_house(self, url: str) -> bool:
        """çˆ¬å–å•ä¸ªæˆ¿æº - å•ä¸€èŒè´£ï¼šåªå¤„ç†ä¸€ä¸ªæˆ¿æº"""
        logger.info(f"å¼€å§‹çˆ¬å–æˆ¿æº: {url}")

        # å®‰å…¨å¯¼èˆªåˆ°æˆ¿æºé¡µ
        if not await anti_crawler.safe_navigate(self.page, url):
            return False

        # æå–æ•°æ®
        data = await data_extractor.extract_data(self.page, url)
        if not data:
            return False

        # å»é‡æ£€æŸ¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.duplicate_checker:
            house_id = data.get('æˆ¿æºç¼–å·', '').strip()
            if self.duplicate_checker.is_duplicate(house_id):
                logger.info(f"è·³è¿‡é‡å¤æˆ¿æº: {house_id}")
                return False

        # ä¿å­˜æ•°æ®
        await self._save_to_csv(data)
        logger.success(f"æˆ¿æºçˆ¬å–æˆåŠŸ: {data.get('æ ‡é¢˜', 'Unknown')}")
        return True

    @handle_errors(default_return=0)
    async def crawl_house_batch(self, house_links: List[str]) -> int:
        """æ‰¹é‡çˆ¬å–æˆ¿æº - å•ä¸€èŒè´£ï¼šåªè´Ÿè´£æ‰¹é‡å¤„ç†é€»è¾‘"""
        success_count = 0

        for i, url in enumerate(house_links, 1):
            logger.progress(f"æˆ¿æºè¿›åº¦: {i}/{len(house_links)}")

            # çˆ¬å–å•ä¸ªæˆ¿æº
            if await self.crawl_single_house(url):
                success_count += 1
                self.stats.record_success()
            else:
                self.stats.record_failure()

            # æ™ºèƒ½å»¶æ—¶
            await anti_crawler.smart_delay()

        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ: æˆåŠŸ{success_count}å¥—ï¼Œå¤±è´¥{len(house_links) - success_count}å¥—")
        return success_count

    async def _crawl_detail_pages(self, house_links: List[str]) -> bool:
        """æ‰¹é‡çˆ¬å–æˆ¿æºè¯¦æƒ…é¡µ - å¤ç”¨æ‰¹é‡å¤„ç†é€»è¾‘"""
        logger.info(f"å¼€å§‹æ‰¹é‡çˆ¬å– {len(house_links)} ä¸ªæˆ¿æºè¯¦æƒ…é¡µ")

        # ç›´æ¥å¤ç”¨æ‰¹é‡å¤„ç†å‡½æ•°
        success_count = await self.crawl_house_batch(house_links)

        return success_count > 0

    async def _prepare_csv(self):
        """å‡†å¤‡CSVæ–‡ä»¶"""
        file_exists = os.path.exists(config.csv_filename)

        # å¦‚æœä¸å­˜åœ¨æˆ–è€…ä¸æ˜¯è¿½åŠ æ¨¡å¼ï¼Œåˆ›å»ºæ–°æ–‡ä»¶
        if not file_exists or not config.append_mode:
            with open(config.csv_filename, 'w', newline='', encoding=config.csv_encoding) as f:
                writer = csv.writer(f)
                writer.writerow(data_extractor.csv_fields)
            logger.csv_created(config.csv_filename)
        else:
            logger.csv_appended(config.csv_filename)

    async def _save_to_csv(self, data: dict):
        """ä¿å­˜æ•°æ®åˆ°CSV - å¸¦è¯¦ç»†æ—¥å¿—"""
        try:
            logger.info(f"ğŸ’¾ å¼€å§‹ä¿å­˜æ•°æ®åˆ°CSV: {config.csv_filename}")

            # ç»Ÿè®¡è¦ä¿å­˜çš„æ•°æ®
            non_empty_data = {k: v for k, v in data.items() if v and v.strip()}
            empty_data_fields = [k for k, v in data.items() if not v or not v.strip()]

            logger.info(f"ğŸ“ ä¿å­˜æ•°æ®ç»Ÿè®¡: å…±{len(data)}ä¸ªå­—æ®µï¼Œæœ‰æ•°æ®{len(non_empty_data)}ä¸ªï¼Œç©ºæ•°æ®{len(empty_data_fields)}ä¸ª")

            # æ˜¾ç¤ºå³å°†ä¿å­˜çš„å…³é”®æ•°æ®
            key_preview = {}
            for field in ['æˆ¿æºç¼–å·', 'æ ‡é¢˜', 'ä»·æ ¼', 'æˆ¿æºæ¦‚å†µ', 'æ›´æ–°æ—¶é—´', 'æŠ¼é‡‘']:
                value = data.get(field, '')
                if value:
                    key_preview[field] = value[:30] + "..." if len(value) > 30 else value
                else:
                    key_preview[field] = "[ç©º]"

            logger.info("ğŸ“‹ å³å°†ä¿å­˜çš„å…³é”®æ•°æ®:")
            for field, value in key_preview.items():
                status = "âœ…" if value != "[ç©º]" else "âŒ"
                logger.info(f"   {status} {field}: {value}")

            # æ‰§è¡Œå®é™…çš„ä¿å­˜æ“ä½œ
            with open(config.csv_filename, 'a', newline='', encoding=config.csv_encoding) as f:
                writer = csv.DictWriter(f, fieldnames=data_extractor.csv_fields)
                writer.writerow(data)

            logger.success(f"âœ… æ•°æ®ä¿å­˜æˆåŠŸ: {data.get('æ ‡é¢˜', 'Unknown')}")

            # å¦‚æœæœ‰ç©ºå­—æ®µï¼Œæ˜¾ç¤ºè­¦å‘Š
            if empty_data_fields:
                logger.warning(f"âš ï¸  ä¿å­˜çš„æ•°æ®ä¸­æœ‰{len(empty_data_fields)}ä¸ªç©ºå­—æ®µ: {', '.join(empty_data_fields[:5])}{'...' if len(empty_data_fields) > 5 else ''}")

        except Exception as e:
            logger.error(f"âŒ CSVä¿å­˜å¤±è´¥: {e}")
            logger.error(f"âŒ å¤±è´¥çš„æ•°æ®æ ‡é¢˜: {data.get('æ ‡é¢˜', 'Unknown')}")
            raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿ä¸Šå±‚å¤„ç†

    def get_stats(self) -> dict:
        """è·å–çˆ¬å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.get_stats()


# ä¾¿æ·å‡½æ•°
async def crawl_anjuke_from_list(max_pages: int = None, max_houses_per_page: int = None) -> bool:
    """ä¸€é”®çˆ¬å–å®‰å±…å®¢æˆ¿æºä¿¡æ¯ - ä»åˆ—è¡¨é¡µå¼€å§‹ï¼Œä½¿ç”¨.envé»˜è®¤é…ç½®

    Args:
        max_pages: æœ€å¤§çˆ¬å–åˆ—è¡¨é¡µæ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨.envé…ç½®ï¼‰
        max_houses_per_page: æ¯é¡µæœ€å¤§æˆ¿æºæ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨.envé…ç½®ï¼‰

    Returns:
        bool: çˆ¬å–æ˜¯å¦æˆåŠŸ
    """
    async with AnjukeCrawler() as crawler:
        return await crawler.crawl_from_list_pages(max_pages, max_houses_per_page)


async def crawl_anjuke_from_urls(urls: List[str]) -> bool:
    """ä»ç»™å®šURLåˆ—è¡¨æ‰¹é‡çˆ¬å–

    Args:
        urls: æˆ¿æºè¯¦æƒ…é¡µURLåˆ—è¡¨

    Returns:
        bool: çˆ¬å–æ˜¯å¦æˆåŠŸ
    """
    async with AnjukeCrawler() as crawler:
        return await crawler._crawl_detail_pages(urls)


if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œçˆ¬è™« - ä»åˆ—è¡¨é¡µå¼€å§‹
    from logger import logger

    logger.info("ğŸš€ å¯åŠ¨å®‰å±…å®¢çˆ¬è™« - é€é¡µå¤„ç†æ¨¡å¼")
    logger.info(f"ğŸ“‹ é…ç½®: åˆ—è¡¨é¡µæ•°={config.max_pages}, æ¯é¡µæˆ¿æºæ•°={config.max_houses_per_page}")
    logger.info(f"ğŸ¯ å·¥ä½œæ¨¡å¼: åŠ è½½ä¸€é¡µæˆ¿æºåˆ—è¡¨â†’çˆ¬å–è¯¥é¡µæ‰€æœ‰è¯¦æƒ…")
    logger.info(f"âš™ï¸  å»¶æ—¶é…ç½®: {config.crawl_delay}ç§’, è¶…æ—¶: {config.browser_timeout}ms")
    logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {config.csv_filename}")

    async def run_crawler():
        # ä½¿ç”¨.envé»˜è®¤é…ç½®ï¼Œæ— éœ€ä¼ é€’å‚æ•°
        success = await crawl_anjuke_from_list()

        if success:
            logger.success("âœ… çˆ¬è™«æ‰§è¡ŒæˆåŠŸ!")
        else:
            logger.error("âŒ çˆ¬è™«æ‰§è¡Œå¤±è´¥!")

    # è¿è¡Œçˆ¬è™«
    asyncio.run(run_crawler())